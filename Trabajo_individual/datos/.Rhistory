}
cuadrado(9)
3**2
cuadrado(8)
cuadrado(7)
Cuadrado(1)
potencia = function(x,n){
return(x**n)
}
potencia = function(x,n){
return(x**n)
}
return(x**n)
potencia(7,3)
potencia(8,5)
Potencia(1,1)
potencia(8,7)
install.packages("plotly")
library(plotly)
library(plotly)
library(plotly)
library(plotly)
library(plumber)
detach("package:plumber", unload = TRUE)
library(raster)
library(plotly)
USPersonalExpenditure <- data.frame("Categorie"=rownames(USPersonalExpenditure), USPersonalExpenditure)
x = 1
x
x <- 2
x
View(USPersonalExpenditure)
USPersonalExpenditure
USPersonalExpenditure[,'Categorie']
c(1,2,3)
c("a","b","c")
USPersonalExpenditure[,c('Categorie','X1940')]
data = USPersonalExpenditure[,c('Categorie', 'X1960')]
data
fig = plotly::plot_ly(data, labels = ~Categorie, values = ~X1960, type = 'pie')
fig = fig %>% layout(title = 'United States Personal Expenditures by Categories in 1960',
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
fig
# Load the plotly library
library(plotly)
# Create sample data
data <- data.frame(
category = c("A", "B", "C", "D"),
values = c(23, 17, 35, 29)
)
# Create a bar chart
fig <- plot_ly(
data,
x = ~category,
y = ~values,
type = 'bar',
marker = list(color = 'rgba(58, 71, 80, 0.6)', line = list(color = 'rgba(58, 71, 80, 1.0)', width = 1.5))
)
# Add titles
fig <- fig %>%
layout(
title = "Sample Bar Chart",
xaxis = list(title = "Category"),
yaxis = list(title = "Values")
)
# Display the plot
fig
setwd("D:/Doctorado/Experimental")
library(lmtest)
library(agricolae)
data = read.csv(file = "bauermcnett.csv",sep = ';')
str(data)
data$Especie <- as.factor(data$Especie)
data$Tratamiento <- as.factor(data$Tratamiento)
data$Malla <- as.factor(data$Malla)
str(data)
res.aov = aov(WRV ~ Especie + Tratamiento + Malla , data = data)
print(reas.aov)
# Si hay normalidad en nuestros datos
aov_residuals = residuals(res.aov)
shapiro.test(aov_residuals)
# Homosteasis
#se mantiene la hipotesis de homogeneidad en agrupamiento por especie
bartlett.test(WRV ~ Especie, data = data)
#se mantiene la hipotesis de homogeneidad en agrupamiento por tratamiento
bartlett.test(WRV ~ Tratamiento, data = data)
# si se puede rechazar la hipotesis nula de homogeneidad de varianzas
bartlett.test(WRV ~ Malla, data = data)
#Durbin-Watson test
lmtest::dwtest(res.aov)
TukeyMetodo <- HSD.test(res.aov, "Malla", alpha = 0.05, group = T)
TukeyMetodo
# 1. Histograma de residuos
ggplot(data.frame(aov_residuals), aes(x = aov_residuals)) +
geom_histogram(binwidth = 0.5, color = "black", fill = "lightblue") +
labs(title = "Histograma de Residuos", x = "Residuos", y = "Frecuencia") +
theme_minimal()
# 2. QQ-plot para verificar normalidad
qqPlot(aov_residuals, main = "QQ-Plot de Residuos")
library(ggplot2) # Para la creación de gráficos
library(car) # Para la verificación de suposiciones
install.packages("car")
setwd("D:/Doctorado/Experimental")
library(lmtest)
library(agricolae)
library(ggplot2) # Para la creación de gráficos
library(car) # Para la verificación de suposiciones
data = read.csv(file = "bauermcnett.csv",sep = ';')
str(data)
data$Especie <- as.factor(data$Especie)
data$Tratamiento <- as.factor(data$Tratamiento)
data$Malla <- as.factor(data$Malla)
str(data)
res.aov = aov(WRV ~ Especie + Tratamiento + Malla , data = data)
print(reas.aov)
print(res.aov)
# Si hay normalidad en nuestros datos
aov_residuals = residuals(res.aov)
shapiro.test(aov_residuals)
# Homosteasis
#se mantiene la hipotesis de homogeneidad en agrupamiento por especie
bartlett.test(WRV ~ Especie, data = data)
#se mantiene la hipotesis de homogeneidad en agrupamiento por tratamiento
bartlett.test(WRV ~ Tratamiento, data = data)
# si se puede rechazar la hipotesis nula de homogeneidad de varianzas
bartlett.test(WRV ~ Malla, data = data)
#Durbin-Watson test
lmtest::dwtest(res.aov)
TukeyMetodo <- HSD.test(res.aov, "Malla", alpha = 0.05, group = T)
TukeyMetodo
# 1. Histograma de residuos
ggplot(data.frame(aov_residuals), aes(x = aov_residuals)) +
geom_histogram(binwidth = 0.5, color = "black", fill = "lightblue") +
labs(title = "Histograma de Residuos", x = "Residuos", y = "Frecuencia") +
theme_minimal()
# 2. QQ-plot para verificar normalidad
qqPlot(aov_residuals, main = "QQ-Plot de Residuos")
# 3. Gráfico de dispersión de residuos vs valores ajustados
fitted_values <- fitted(res.aov)
ggplot(data.frame(Fitted = fitted_values, Residuals = aov_residuals), aes(x = Fitted, y = Residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Gráfico de Residuos vs Valores Ajustados", x = "Valores Ajustados", y = "Residuos") +
theme_minimal()
# 4. Gráfico de medias con intervalo de confianza para el análisis de Tukey
Tukey_plot_data <- data.frame(Treatment = rownames(TukeyMetodo$groups),
Mean = TukeyMetodo$means[,1],
Lower = TukeyMetodo$means[,1] - TukeyMetodo$means[,2],
Upper = TukeyMetodo$means[,1] + TukeyMetodo$means[,2])
ggplot(Tukey_plot_data, aes(x = Treatment, y = Mean)) +
geom_point(size = 3) +
geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2) +
labs(title = "Intervalo de Confianza de Tukey para las Medias", x = "Tratamiento (Malla)", y = "Media WRV") +
theme_minimal()
View(data)
View(data)
setwd("D:/Doctorado/Experimental")
library(lmtest)
library(agricolae)
library(ggplot2) # Para la creación de gráficos
library(car) # Para la verificación de suposiciones
data = read.csv(file = "bauermcnett.csv",sep = ';')
str(data)
data = read.csv(file = "bauermcnett.csv",sep = ';')
data
setwd("D:/Proyectos_GitHub/Diseno_experimental/tarea4/")
library(lmtest)
library(agricolae)
library(ggplot2) # Para la creación de gráficos
library(car) # Para la verificación de suposiciones
data = read.csv(file = "bauermcnett.csv",sep = ';')
str(data)
data$Especie <- as.factor(data$Especie)
data$Tratamiento <- as.factor(data$Tratamiento)
data$Malla <- as.factor(data$Malla)
str(data)
res.aov = aov(peso ~ Especie + Tratamiento + Malla , data = data)
print(res.aov)
res.aov = aov(peso ~ Especie + Tratamiento + Malla , data = data)
str(data)
res.aov = aov(Peso ~ Especie + Tratamiento + Malla , data = data)
print(res.aov)
res.aov = aov(Peso ~ Especie + Tratamiento + Malla , data = data)
print(res.aov)
print(res.aov)
# Homosteasis
#se mantiene la hipotesis de homogeneidad en agrupamiento por especie
bartlett.test(Peso ~ Especie, data = data)
#se mantiene la hipotesis de homogeneidad en agrupamiento por tratamiento
bartlett.test(Peso ~ Tratamiento, data = data)
# si se puede rechazar la hipotesis nula de homogeneidad de varianzas
bartlett.test(Peso ~ Malla, data = data)
# Homosteasis
#se mantiene la hipotesis de homogeneidad en agrupamiento por especie
bartlett.test(Peso ~ Especie, data = data)
bartlett.test(Peso ~ Especie, data = data)
bartlett.test(Peso ~ Tratamiento, data = data)
bartlett.test(Peso ~ Malla, data = data)
#Durbin-Watson test
lmtest::dwtest(res.aov)
print(TukeyMetodo)
setwd("D:/Proyectos_GitHub/Diseno_experimental/tarea4/")
library(lmtest)
library(agricolae)
library(ggplot2) # Para la creación de gráficos
library(car) # Para la verificación de suposiciones
data = read.csv(file = "bauermcnett.csv",sep = ';')
str(data)
data$Especie <- as.factor(data$Especie)
data$Tratamiento <- as.factor(data$Tratamiento)
data$Malla <- as.factor(data$Malla)
str(data)
res.aov = aov(Peso ~ Especie + Tratamiento + Malla , data = data)
print(res.aov)
# Si hay normalidad en nuestros datos
aov_residuals = residuals(res.aov)
shapiro.test(aov_residuals)
# Homosteasis
bartlett.test(Peso ~ Especie, data = data)
bartlett.test(Peso ~ Tratamiento, data = data)
bartlett.test(Peso ~ Malla, data = data)
#Durbin-Watson test
lmtest::dwtest(res.aov)
#Tukey DBCA
TukeyMetodo <- HSD.test(res.aov, "Malla", alpha = 0.05, group = T)
print(TukeyMetodo)
# 2. QQ-plot para verificar normalidad
qqPlot(aov_residuals, main = "QQ-Plot de Residuos")
fitted_values <- fitted(res.aov)
ggplot(data.frame(Fitted = fitted_values, Residuals = aov_residuals), aes(x = Fitted, y = Residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Gráfico de Residuos vs Valores Ajustados", x = "Valores Ajustados", y = "Residuos") +
theme_minimal()
# 4. Gráfico de medias con intervalo de confianza para el análisis de Tukey
Tukey_plot_data <- data.frame(Treatment = rownames(TukeyMetodo$groups),
Mean = TukeyMetodo$means[,1],
Lower = TukeyMetodo$means[,1] - TukeyMetodo$means[,2],
Upper = TukeyMetodo$means[,1] + TukeyMetodo$means[,2])
ggplot(Tukey_plot_data, aes(x = Treatment, y = Mean)) +
geom_point(size = 3) +
geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2) +
labs(title = "Intervalo de Confianza de Tukey para las Medias", x = "Tratamiento (Malla)", y = "Media WRV") +
theme_minimal()
ggplot(Tukey_plot_data, aes(x = Treatment, y = Mean)) +
geom_point(size = 3) +
geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2) +
labs(title = "Intervalo de Confianza de Tukey para las Medias", x = "Tratamiento (Malla)", y = "Media Peso") +
theme_minimal()
aov_residuals = residuals(res.aov)
shapiro.test(aov_residuals)
res.aov = aov(Peso ~ Especie + Tratamiento + Malla , data = data)
print(res.aov)
res.aov = aov(Peso ~ Especie + Tratamiento + Malla , data = data)
print(res.aov)
print(res.aov)
#Durbin-Watson test
lmtest::dwtest(res.aov)
#Durbin-Watson test
dw = lmtest::dwtest(res.aov)
print(dw)
# Cargar librerías necesarias
library(dplyr)
library(agricolae)
library(car) # Para la verificación de suposiciones
# Crear el dataframe original con los valores medios y la desviación estándar para cada variable
data <- data.frame(
Intensidad_Pastoreo = c("G0", "G0.7", "G1.2", "G1.6"),
DOC_mean = c(207.05, 177.74, 170.76, 151.64),
DOC_sd = c(0.68, 8.05, 48.67, 11.03),
P_mean = c(0.86, 1.74, 1.14, 1.42),
P_sd = c(0.04, 0.08, 0.04, 0.12),
NO3_mean = c(0.07, 0.12, 0.10, 0.08),
NO3_sd = c(0.004, 0.006, 0.008, 0.01),
NH4_mean = c(1.26, 1.13, 1.22, 1.37),
NH4_sd = c(0.08, 0.06, 0.06, 0.11)
)
# Crear un dataframe vacío para almacenar los resultados
data_expanded <- data.frame()
# Iterar sobre cada fila del dataframe original y crear las repeticiones
for (i in 1:nrow(data)) {
# Para cada variable (DOC, P, NO3-N, NH4+-N), generar las tres repeticiones
temp <- data.frame(
Fecha = rep("2013", 12),  # Ajuste de fecha para el año del estudio
Intensidad_Pastoreo = rep(data$Intensidad_Pastoreo[i], 12),
Variable = rep(c("DOC", "P", "NO3-N", "NH4+-N"), each = 3),
Valor = c(
data$DOC_mean[i] - data$DOC_sd[i], data$DOC_mean[i], data$DOC_mean[i] + data$DOC_sd[i],
data$P_mean[i] - data$P_sd[i], data$P_mean[i], data$P_mean[i] + data$P_sd[i],
data$NO3_mean[i] - data$NO3_sd[i], data$NO3_mean[i], data$NO3_mean[i] + data$NO3_sd[i],
data$NH4_mean[i] - data$NH4_sd[i], data$NH4_mean[i], data$NH4_mean[i] + data$NH4_sd[i]
)
)
# Añadir los datos temporales al dataframe expandido
data_expanded <- rbind(data_expanded, temp)
}
# Mostrar el dataframe expandido
print(data_expanded)
View(data_expanded)
# Cargar librerías necesarias
library(dplyr)
library(agricolae)
library(car) # Para la verificación de suposiciones
# Crear el dataframe original con los valores medios y la desviación estándar para cada variable
data <- data.frame(
Intensidad_Pastoreo = c("G0", "G0.7", "G1.2", "G1.6"),
DOC_mean = c(207.05, 177.74, 170.76, 151.64),
DOC_sd = c(0.68, 8.05, 48.67, 11.03),
P_mean = c(0.86, 1.74, 1.14, 1.42),
P_sd = c(0.04, 0.08, 0.04, 0.12),
NO3_mean = c(0.07, 0.12, 0.10, 0.08),
NO3_sd = c(0.004, 0.006, 0.008, 0.01),
NH4_mean = c(1.26, 1.13, 1.22, 1.37),
NH4_sd = c(0.08, 0.06, 0.06, 0.11)
)
# Crear un dataframe vacío para almacenar los resultados
data_expanded <- data.frame()
# Iterar sobre cada fila del dataframe original y crear las repeticiones
for (i in 1:nrow(data)) {
# Para cada variable (DOC, P, NO3-N, NH4+-N), generar las tres repeticiones
temp <- data.frame(
Fecha = rep("2013", 12),  # Ajuste de fecha para el año del estudio
Intensidad_Pastoreo = rep(data$Intensidad_Pastoreo[i], 12),
Psuelo = rep(c("DOC", "P", "NO3-N", "NH4+-N"), each = 3),
Valor = c(
data$DOC_mean[i] - data$DOC_sd[i], data$DOC_mean[i], data$DOC_mean[i] + data$DOC_sd[i],
data$P_mean[i] - data$P_sd[i], data$P_mean[i], data$P_mean[i] + data$P_sd[i],
data$NO3_mean[i] - data$NO3_sd[i], data$NO3_mean[i], data$NO3_mean[i] + data$NO3_sd[i],
data$NH4_mean[i] - data$NH4_sd[i], data$NH4_mean[i], data$NH4_mean[i] + data$NH4_sd[i]
)
)
# Añadir los datos temporales al dataframe expandido
data_expanded <- rbind(data_expanded, temp)
}
# Mostrar el dataframe expandido
print(data_expanded)
View(data_expanded)
data_expanded$Fecha = NULL
# Ordenar el dataframe expandido por la columna "Variable"
data_expanded <- arrange(data_expanded, Psuelo)
# Reorganizar las columnas para que "Variable" sea la primera
data_expanded <- select(data_expanded, Psuelo, everything())
# Mostrar el dataframe resultante
print(data_expanded)
# Asegurarse de que los factores estén correctamente definidos
data_expanded$Intensidad_Pastoreo <- as.factor(data_expanded$Intensidad_Pastoreo)
data_expanded$Psuelo <- as.factor(data_expanded$Psuelo)
#Anova para DBCA
res.aov <- aov(Valor ~ Tratamiento+Variable, data = data_expanded)
summary(res.aov)
#Anova para DBCA
res.aov <- aov(Valor ~ Intesidad_Pastoreo+Psuelo, data = data_expanded)
#Anova para DBCA
res.aov <- aov(Valor ~ Intensidad_Pastoreo+Psuelo, data = data_expanded)
summary(res.aov)
# Si hay normalidad en nuestros datos
aov_residuals = residuals(res.aov)
shapiro.test(aov_residuals)
#QQ-plot para verificar normalidad
qqPlot(aov_residuals, main = "QQ-Plot de Residuos")
bartlett.test(Valor ~ Intensidad_Pastoreoe, data = data_expanded)
bartlett.test(Valor ~ Psuelo, data = data_expanded)
bartlett.test(Valor ~ Intensidad_Pastoreo, data = data_expanded)
#Durbin-Watson test
lmtest::dwtest(res.aov)
tukeyMetodo1 = HSD.test(res.aov,"Tratamiento", alpha =0.05, group=T)
tukeyMetodo1 = HSD.test(res.aov,"Intensidad_Pastoreo", alpha =0.05, group=T)
plot(tukeyMetodo1,variation="IQR")
tukeyMetodo2 = HSD.test(res.aov,"Psuelo", alpha =0.05, group=T)
plot(tukeyMetodo2,variation="IQR")
# Cargar la librería ggplot2
library(ggplot2)
# Calcular los valores promedio por tratamiento y variable
intEf <- aggregate(Valor ~ Intensidad_Pastoreo + Psuelo, FUN = mean, data = data_expanded)
# Cargar la librería ggplot2
library(ggplot2)
# Calcular los valores promedio por tratamiento y variable
intEf <- aggregate(Valor ~ Intensidad_Pastoreo + Psuelo, FUN = mean, data = data_expanded)
# Crear el gráfico con las unidades añadidas
effects_interaction <- ggplot(intEf, aes(x = Intensidad_Pastoreo, y = Valor, color = Psuelo)) +
geom_point() +
geom_line(aes(group = Psuelo)) +
labs(
y = "Valor (kg N ha⁻¹)",
x = "Intensidad de Pastoreo",
color = "Variable (Unidad)"
) +
scale_color_manual(
values = c("DOC" = "blue", "P" = "green", "NO3-N" = "red", "NH4+-N" = "purple"),
labels = c(
"DOC (mg/L)",
"P (mg/L)",
"NO3-N (mg/L)",
"NH4+-N (mg/L)"
)
)
# Mostrar el gráfico
print(effects_interaction)
# Cargar librerías necesarias
library(dplyr)
library(agricolae)
library(car) # Para la verificación de suposiciones
# Crear el dataframe original con los valores medios y la desviación estándar para cada variable
data <- data.frame(
Intensidad_Pastoreo = c("G0", "G0.7", "G1.2", "G1.6"),
P_mean = c(0.86, 1.74, 1.14, 1.42),
P_sd = c(0.04, 0.08, 0.04, 0.12),
NO3_mean = c(0.07, 0.12, 0.10, 0.08),
NO3_sd = c(0.004, 0.006, 0.008, 0.01),
NH4_mean = c(1.26, 1.13, 1.22, 1.37),
NH4_sd = c(0.08, 0.06, 0.06, 0.11)
)
# Crear un dataframe vacío para almacenar los resultados
data_expanded <- data.frame()
# Iterar sobre cada fila del dataframe original y crear las repeticiones
for (i in 1:nrow(data)) {
# Para cada variable (P, NO3-N, NH4+-N), generar las tres repeticiones
temp <- data.frame(
Fecha = rep("2013", 9),  # Ajuste de fecha para el año del estudio
Intensidad_Pastoreo = rep(data$Intensidad_Pastoreo[i], 9),
Psuelo = rep(c("P", "NO3-N", "NH4+-N"), each = 3),
Valor = c(
data$P_mean[i] - data$P_sd[i], data$P_mean[i], data$P_mean[i] + data$P_sd[i],
data$NO3_mean[i] - data$NO3_sd[i], data$NO3_mean[i], data$NO3_mean[i] + data$NO3_sd[i],
data$NH4_mean[i] - data$NH4_sd[i], data$NH4_mean[i], data$NH4_mean[i] + data$NH4_sd[i]
)
)
# Añadir los datos temporales al dataframe expandido
data_expanded <- rbind(data_expanded, temp)
}
# Mostrar el dataframe expandido
print(data_expanded)
data_expanded$Fecha = NULL
# Ordenar el dataframe expandido por la columna "Variable"
data_expanded <- arrange(data_expanded, Psuelo)
# Reorganizar las columnas para que "Variable" sea la primera
data_expanded <- select(data_expanded, Psuelo, everything())
# Mostrar el dataframe resultante
print(data_expanded)
# Asegurarse de que los factores estén correctamente definidos
data_expanded$Intensidad_Pastoreo <- as.factor(data_expanded$Intensidad_Pastoreo)
data_expanded$Psuelo <- as.factor(data_expanded$Psuelo)
# Anova para DBCA
res.aov <- aov(Valor ~ Intensidad_Pastoreo + Psuelo, data = data_expanded)
summary(res.aov)
# Si hay normalidad en nuestros datos
aov_residuals = residuals(res.aov)
shapiro.test(aov_residuals)
# QQ-plot para verificar normalidad
qqPlot(aov_residuals, main = "QQ-Plot de Residuos")
bartlett.test(Valor ~ Intensidad_Pastoreo, data = data_expanded)
bartlett.test(Valor ~ Psuelo, data = data_expanded)
# Durbin-Watson test
lmtest::dwtest(res.aov)
tukeyMetodo1 = HSD.test(res.aov, "Intensidad_Pastoreo", alpha = 0.05, group = T)
plot(tukeyMetodo1, variation = "IQR")
tukeyMetodo2 = HSD.test(res.aov, "Psuelo", alpha = 0.05, group = T)
plot(tukeyMetodo2, variation = "IQR")
# Cargar la librería ggplot2
library(ggplot2)
# Calcular los valores promedio por tratamiento y variable
intEf <- aggregate(Valor ~ Intensidad_Pastoreo + Psuelo, FUN = mean, data = data_expanded)
# Crear el gráfico con las unidades añadidas
effects_interaction <- ggplot(intEf, aes(x = Intensidad_Pastoreo, y = Valor, color = Psuelo)) +
geom_point() +
geom_line(aes(group = Psuelo)) +
labs(
y = "Valor (kg N ha⁻¹)",
x = "Intensidad de Pastoreo",
color = "Variable (Unidad)"
) +
scale_color_manual(
values = c("P" = "green", "NO3-N" = "blue", "NH4+-N" = "red"),
labels = c(
"P (mg/L)",
"NO3-N (mg/L)",
"NH4+-N (mg/L)"
)
)
# Mostrar el gráfico
print(effects_interaction)
# Anova para Factorial 2
res.aov <- aov(Valor ~ Intensidad_Pastoreo + Psuelo + Intensidad_Pastoreo * Psuelo, data = data_expanded)
summary(res.aov)
# Anova para DBCA
res.aov <- aov(Valor ~ Intensidad_Pastoreo + Psuelo, data = data_expanded)
summary(res.aov)
# Asegurarse de que los factores estén correctamente definidos
data_expanded$Intensidad_Pastoreo <- as.factor(data_expanded$Intensidad_Pastoreo)
data_expanded$Psuelo <- as.factor(data_expanded$Psuelo)
# Anova para DBCA
res.aov <- aov(Valor ~ Intensidad_Pastoreo + Psuelo, data = data_expanded)
summary(res.aov)
# Si hay normalidad en nuestros datos
aov_residuals = residuals(res.aov)
shapiro.test(aov_residuals)
# QQ-plot para verificar normalidad
qqPlot(aov_residuals, main = "QQ-Plot de Residuos")
bartlett.test(Valor ~ Intensidad_Pastoreo, data = data_expanded)
bartlett.test(Valor ~ Psuelo, data = data_expanded)
# Durbin-Watson test
lmtest::dwtest(res.aov)
tukeyMetodo1 = HSD.test(res.aov, "Intensidad_Pastoreo", alpha = 0.05, group = T)
plot(tukeyMetodo1, variation = "IQR")
tukeyMetodo2 = HSD.test(res.aov, "Psuelo", alpha = 0.05, group = T)
plot(tukeyMetodo2, variation = "IQR")
setwd("~/GitHub/Diseno_experimental/Trabajo_individual/datos")
write.csv(data_expanded,'datos.csv',row.names = F)
# Anova para DBCA
res.aov <- aov(Valor ~ Intensidad_Pastoreo + Psuelo, data = data_expanded)
summary(res.aov)
